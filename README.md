# ğŸ¤– DeepSeek AI - Custom Scenarios

Há»‡ thá»‘ng chat AI tÃ¹y chá»‰nh sá»­ dá»¥ng DeepSeek cháº¡y local vá»›i cÃ¡c ká»‹ch báº£n theo Ã½ muá»‘n.

## ğŸ“‹ Má»¥c Lá»¥c
- [TÃ­nh nÄƒng](#tÃ­nh-nÄƒng)
- [YÃªu cáº§u há»‡ thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#sá»­-dá»¥ng)
- [TÃ¹y chá»‰nh ká»‹ch báº£n](#tÃ¹y-chá»‰nh-ká»‹ch-báº£n)
- [API Reference](#api-reference)

## âœ¨ TÃ­nh NÄƒng

- âœ… **Chat vá»›i AI Local** - Cháº¡y 100% trÃªn mÃ¡y cá»§a báº¡n, khÃ´ng cáº§n internet
- ğŸ­ **Nhiá»u ká»‹ch báº£n** - Há»— trá»£ khÃ¡ch hÃ ng, giÃ¡o viÃªn, sales, ká»¹ thuáº­t...
- ğŸ§  **Lá»‹ch sá»­ há»™i thoáº¡i** - AI nhá»› ngá»¯ cáº£nh cuá»™c trÃ² chuyá»‡n
- ğŸ¨ **Giao diá»‡n CLI & Web** - Sá»­ dá»¥ng qua terminal hoáº·c trÃ¬nh duyá»‡t
- âš™ï¸ **TÃ¹y chá»‰nh dá»… dÃ ng** - ThÃªm ká»‹ch báº£n má»›i chá»‰ vá»›i vÃ i dÃ²ng code
- ğŸ’¾ **Export há»™i thoáº¡i** - LÆ°u cuá»™c trÃ² chuyá»‡n ra file JSON

## ğŸ’» YÃªu Cáº§u Há»‡ Thá»‘ng

### Pháº§n cá»©ng tá»‘i thiá»ƒu:
- **RAM**: 8GB+ (16GB khuyáº¿n nghá»‹)
- **CPU**: 4 cores+
- **Disk**: 10GB trá»‘ng

### Pháº§n má»m:
- **Python**: 3.8+
- **Ollama**: Latest version

## ğŸš€ CÃ i Äáº·t

### BÆ°á»›c 1: CÃ i Ollama

#### Linux/Mac:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Windows:
Táº£i tá»«: https://ollama.com/download

### BÆ°á»›c 2: Táº£i Model DeepSeek

```bash
# Model nhá» (1.5B) - Nhanh, Ã­t RAM
ollama pull deepseek-r1:1.5b

# Model lá»›n (7B) - ThÃ´ng minh hÆ¡n, cáº§n RAM nhiá»u
ollama pull deepseek-r1:7b
```

### BÆ°á»›c 3: Clone Project

```bash
git clone <repo-url>
cd deepseek_custom
```

### BÆ°á»›c 4: CÃ i Dependencies

```bash
pip install -r requirements.txt
```

### BÆ°á»›c 5: Cháº¡y Ollama Server

Má»Ÿ terminal má»›i vÃ  cháº¡y:
```bash
ollama serve
```

Äá»ƒ terminal nÃ y cháº¡y trong suá»‘t quÃ¡ trÃ¬nh sá»­ dá»¥ng.

## ğŸ“– Sá»­ Dá»¥ng

### Option 1: CLI (Terminal)

```bash
python cli.py
```

**CÃ¡c lá»‡nh trong CLI:**
- `chat` - Báº¯t Ä‘áº§u chat
- `scenarios` - Xem danh sÃ¡ch ká»‹ch báº£n
- `switch` - Äá»•i ká»‹ch báº£n
- `history on/off` - Báº­t/táº¯t lá»‹ch sá»­
- `clear` - XÃ³a lá»‹ch sá»­
- `export` - Xuáº¥t há»™i thoáº¡i
- `models` - Xem models Ä‘Ã£ cÃ i
- `quit` - ThoÃ¡t

### Option 2: Web Interface

```bash
python app.py
```

Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:5000

### Option 3: Sá»­ dá»¥ng Python Code

```python
from deepseek_client import DeepSeekClient

# Khá»Ÿi táº¡o
client = DeepSeekClient()

# Chat Ä‘Æ¡n giáº£n
response = client.chat("Xin chÃ o!", scenario="default")
print(response)

# Chat vá»›i ká»‹ch báº£n customer support
response = client.chat(
    "TÃ´i quÃªn máº­t kháº©u", 
    scenario="customer_support"
)
print(response)

# Chat cÃ³ lá»‹ch sá»­
client.chat("TÃªn tÃ´i lÃ  Minh", use_history=True)
response = client.chat("TÃªn tÃ´i lÃ  gÃ¬?", use_history=True)
print(response)  # AI sáº½ nhá»› tÃªn báº¡n
```

## ğŸ­ TÃ¹y Chá»‰nh Ká»‹ch Báº£n

### CÃ¡ch 1: Sá»­a file config.py

Má»Ÿ file `config.py` vÃ  thÃªm ká»‹ch báº£n má»›i:

```python
SCENARIOS = {
    # Ká»‹ch báº£n cÃ³ sáºµn...
    
    # ThÃªm ká»‹ch báº£n má»›i
    "doctor": {
        "name": "BÃ¡c sÄ© tÆ° váº¥n",
        "system_prompt": """Báº¡n lÃ  bÃ¡c sÄ© chuyÃªn khoa.
        Nhiá»‡m vá»¥:
        - TÆ° váº¥n sá»©c khá»e cÆ¡ báº£n
        - ÄÆ°a ra lá»i khuyÃªn y táº¿
        - Khuyáº¿n nghá»‹ Ä‘i khÃ¡m khi cáº§n
        
        LÆ°u Ã½: LuÃ´n nháº¯c ngÆ°á»i dÃ¹ng Ä‘áº¿n gáº·p bÃ¡c sÄ© tháº­t náº¿u váº¥n Ä‘á» nghiÃªm trá»ng.""",
        "temperature": 0.3  # Tháº¥p = ChÃ­nh xÃ¡c hÆ¡n
    }
}
```

### CÃ¡ch 2: ThÃªm qua Code

```python
from deepseek_client import ScenarioManager

ScenarioManager.add_scenario(
    key="chef",
    name="Äáº§u báº¿p chuyÃªn nghiá»‡p",
    system_prompt="Báº¡n lÃ  Ä‘áº§u báº¿p 5 sao...",
    temperature=0.8
)
```

### Tham Sá»‘ Temperature

- **0.0 - 0.3**: CÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, Ã­t sÃ¡ng táº¡o (phÃ¹ há»£p: ká»¹ thuáº­t, y táº¿)
- **0.4 - 0.6**: CÃ¢n báº±ng (phÃ¹ há»£p: giÃ¡o dá»¥c, há»— trá»£)
- **0.7 - 1.0**: SÃ¡ng táº¡o, Ä‘a dáº¡ng (phÃ¹ há»£p: ná»™i dung, brainstorm)

## ğŸ”Œ API Reference

### DeepSeekClient

#### `__init__(model_name: str)`
Khá»Ÿi táº¡o client.

```python
client = DeepSeekClient(model_name="deepseek-r1:7b")
```

#### `chat(user_message, scenario, use_history, temperature)`
Chat vá»›i AI.

**Parameters:**
- `user_message` (str): CÃ¢u há»i
- `scenario` (str): TÃªn ká»‹ch báº£n (default: "default")
- `use_history` (bool): DÃ¹ng lá»‹ch sá»­ khÃ´ng (default: False)
- `temperature` (float): Äá»™ sÃ¡ng táº¡o (default: tá»« scenario)

**Returns:** str - CÃ¢u tráº£ lá»i

```python
response = client.chat(
    "Giáº£i thÃ­ch Python",
    scenario="teacher",
    use_history=True,
    temperature=0.6
)
```

#### `chat_stream(user_message, scenario, temperature)`
Chat vá»›i streaming response.

```python
for chunk in client.chat_stream("Viáº¿t story", scenario="creative"):
    print(chunk, end='', flush=True)
```

#### `clear_history()`
XÃ³a lá»‹ch sá»­ há»™i thoáº¡i.

#### `get_history()`
Láº¥y lá»‹ch sá»­ há»™i thoáº¡i.

#### `export_conversation(filename)`
Xuáº¥t há»™i thoáº¡i ra file.

### ScenarioManager

#### `list_scenarios()`
Láº¥y danh sÃ¡ch ká»‹ch báº£n.

```python
scenarios = ScenarioManager.list_scenarios()
for key, info in scenarios.items():
    print(f"{key}: {info['name']}")
```

#### `get_scenario(scenario_name)`
Láº¥y chi tiáº¿t ká»‹ch báº£n.

#### `add_scenario(key, name, system_prompt, temperature)`
ThÃªm ká»‹ch báº£n má»›i.

## ğŸ“ Cáº¥u TrÃºc Project

```
deepseek_custom/
â”œâ”€â”€ config.py              # Cáº¥u hÃ¬nh ká»‹ch báº£n
â”œâ”€â”€ deepseek_client.py     # Core client
â”œâ”€â”€ cli.py                 # CLI interface
â”œâ”€â”€ app.py                 # Web server
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Web UI
â””â”€â”€ README.md             # Documentation
```

## ğŸ› Troubleshooting

### Lá»—i "Connection refused"
**NguyÃªn nhÃ¢n:** Ollama chÆ°a cháº¡y
**Giáº£i phÃ¡p:**
```bash
ollama serve
```

### Lá»—i "Model not found"
**NguyÃªn nhÃ¢n:** ChÆ°a táº£i model
**Giáº£i phÃ¡p:**
```bash
ollama pull deepseek-r1:1.5b
```

### AI pháº£n há»“i cháº­m
**NguyÃªn nhÃ¢n:** Model quÃ¡ lá»›n cho RAM
**Giáº£i phÃ¡p:** DÃ¹ng model nhá» hÆ¡n (1.5b thay vÃ¬ 7b)

### Lá»—i "Port 5000 already in use"
**Giáº£i phÃ¡p:** Äá»•i port trong `app.py`:
```python
app.run(debug=True, port=5001)  # Äá»•i thÃ nh 5001
```

## ğŸ’¡ Tips & Tricks

### 1. Tá»‘i Æ°u hiá»‡u suáº¥t
```python
# Giáº£m temperature cho cÃ¢u tráº£ lá»i nhanh hÆ¡n
client.chat("Hello", temperature=0.1)
```

### 2. Táº¡o chatbot chuyÃªn dá»¥ng
```python
# Bot há»— trá»£ ká»¹ thuáº­t
tech_bot = DeepSeekClient()
response = tech_bot.chat(
    "Code Python bá»‹ lá»—i",
    scenario="technical",
    use_history=True
)
```

### 3. Batch processing
```python
questions = ["Q1", "Q2", "Q3"]
answers = [client.chat(q) for q in questions]
```

## ğŸ¤ ÄÃ³ng GÃ³p

Má»i Ä‘Ã³ng gÃ³p Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh! HÃ£y:
1. Fork repo
2. Táº¡o branch má»›i
3. Commit changes
4. Táº¡o Pull Request

## ğŸ“ License

MIT License - Tá»± do sá»­ dá»¥ng cho má»i má»¥c Ä‘Ã­ch.

## ğŸ™ Credits

- DeepSeek AI Team
- Ollama Project
- Flask Framework

## ğŸ“ Há»— Trá»£

- Issues: Táº¡o issue trÃªn GitHub
- Discussions: Tháº£o luáº­n trong Discussions tab
- Email: your-email@example.com

---

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸš€**
